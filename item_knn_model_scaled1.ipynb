{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "914e1f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:1863: RuntimeWarning: overflow encountered in cast\n",
      "  casted = dtype.type(element)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to score 5000 validation searches: 662.26 seconds\n",
      "Validation NDCG@5: 0.1792\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def ndcg_at_k(r, k=5):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size == 0:\n",
    "        return 0.0\n",
    "    dcg = np.sum((2**r - 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    ideal_r = np.sort(r)[::-1]\n",
    "    idcg = np.sum((2**ideal_r - 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# Load training data\n",
    "full_train_df = pd.read_parquet('training_set_processed.parquet')\n",
    "\n",
    "# Sample 10% of the data by search_id (to prevent leakage)\n",
    "sampled_srch_ids = full_train_df['srch_id'].drop_duplicates().sample(frac=0.2, random_state=42)\n",
    "train_df = full_train_df[full_train_df['srch_id'].isin(sampled_srch_ids)].copy()\n",
    "\n",
    "# Split into training and validation set (again, by srch_id to avoid leakage)\n",
    "splitter = GroupShuffleSplit(test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(splitter.split(train_df, groups=train_df['srch_id']))\n",
    "train_data = train_df.iloc[train_idx].copy()\n",
    "val_data = train_df.iloc[val_idx].copy()\n",
    "\n",
    "# Feature columns to use\n",
    "feature_cols = [\n",
    "    'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "    'prop_location_score1', 'prop_location_score2', 'price_usd',\n",
    "    'prop_log_historical_price', 'promotion_flag', 'orig_destination_distance'\n",
    "]\n",
    "\n",
    "# Handle special case: prop_log_historical_price == 0 means missing\n",
    "train_data['prop_log_historical_price'] = train_data['prop_log_historical_price'].replace(0, np.nan)\n",
    "\n",
    "# Add missing value indicator columns\n",
    "for col in ['prop_review_score', 'orig_destination_distance']:\n",
    "    train_data[f'{col}_missing'] = train_data[col].isna().astype(int)\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "# price_per_night\n",
    "train_data['price_per_night'] = train_data['price_usd'] / train_data['srch_length_of_stay'].replace(0, np.nan)\n",
    "\n",
    "# price_per_person\n",
    "group_size = train_data['srch_adults_count'] + train_data['srch_children_count']\n",
    "train_data['price_per_person'] = train_data['price_usd'] / group_size.replace(0, np.nan)\n",
    "\n",
    "# price_ratio_to_mean_in_search\n",
    "mean_price_per_search = train_data.groupby('srch_id')['price_usd'].transform('mean')\n",
    "train_data['price_ratio_to_mean'] = train_data['price_usd'] / mean_price_per_search.replace(0, np.nan)\n",
    "\n",
    "# price × star rating interaction\n",
    "train_data['price_x_star'] = train_data['price_usd'] * train_data['prop_starrating']\n",
    "\n",
    "# same_country_bool\n",
    "train_data['same_country'] = (train_data['visitor_location_country_id'] == train_data['prop_country_id']).astype(int)\n",
    "\n",
    "# Apply imputation\n",
    "train_data['prop_review_score'] = train_data['prop_review_score'].fillna(train_data['prop_review_score'].median())\n",
    "train_data['orig_destination_distance'] = train_data['orig_destination_distance'].fillna(train_data['orig_destination_distance'].mean())\n",
    "train_data['prop_log_historical_price'] = train_data['prop_log_historical_price'].fillna(train_data['prop_log_historical_price'].mean())\n",
    "\n",
    "# Rebuild feature list with missing indicators\n",
    "feature_cols.extend(['price_per_night', 'price_per_person', 'price_ratio_to_mean', 'price_x_star', 'same_country'])\n",
    "feature_cols.extend(['prop_review_score_missing', 'orig_destination_distance_missing'])\n",
    "\n",
    "# Limit training to top 40000 most frequent hotels\n",
    "top_props = train_data['prop_id'].value_counts().head(30000).index\n",
    "train_data = train_data[train_data['prop_id'].isin(top_props)]\n",
    "\n",
    "# Create item profiles\n",
    "item_profiles = train_data[['prop_id'] + feature_cols].drop_duplicates('prop_id').set_index('prop_id')\n",
    "item_profiles.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "item_profiles.fillna(item_profiles.mean(), inplace=True)\n",
    "item_profiles.fillna(0, inplace=True)  # final fallback\n",
    "\n",
    "# Clip extreme values to prevent overflow\n",
    "for col in feature_cols:\n",
    "    item_profiles[col] = np.clip(item_profiles[col], -1e6, 1e6)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "item_profiles_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(item_profiles.astype(np.float64)), index=item_profiles.index, columns=feature_cols)\n",
    "\n",
    "# Similarity matrix\n",
    "similarity_matrix = pd.DataFrame(\n",
    "    cosine_similarity(item_profiles_scaled),\n",
    "    index=item_profiles_scaled.index,\n",
    "    columns=item_profiles_scaled.index\n",
    ")\n",
    "\n",
    "# Precompute top-10 neighbors per hotel\n",
    "precomputed_neighbors = {}\n",
    "for prop in similarity_matrix.index:\n",
    "    sim_series = similarity_matrix.loc[prop].drop(prop, errors='ignore')\n",
    "    top_k = sim_series.nlargest(10)\n",
    "    precomputed_neighbors[prop] = list(top_k.items())\n",
    "similarity_matrix = pd.DataFrame(\n",
    "    cosine_similarity(item_profiles_scaled),\n",
    "    index=item_profiles_scaled.index,\n",
    "    columns=item_profiles_scaled.index\n",
    ")\n",
    "\n",
    "# Click/booking rates\n",
    "click_rate = train_data.groupby('prop_id')['click_bool'].mean().to_dict()\n",
    "book_rate = train_data.groupby('prop_id')['booking_bool'].mean().to_dict()\n",
    "\n",
    "# Scoring function\n",
    "def score_with_knn(val_df, precomputed_neighbors):\n",
    "    results = []\n",
    "    for srch_id, group in val_df.groupby('srch_id'):\n",
    "        props = group['prop_id'].values\n",
    "        scores = {}\n",
    "        for prop in props:\n",
    "            if prop not in similarity_matrix.index:\n",
    "                scores[prop] = 0\n",
    "                continue\n",
    "            sim_series = similarity_matrix[prop].drop(prop, errors='ignore')\n",
    "            top_k = precomputed_neighbors.get(prop, [])\n",
    "            score = 0\n",
    "            for neighbor, sim in top_k.items():\n",
    "                cr = click_rate.get(neighbor, 0)\n",
    "                br = book_rate.get(neighbor, 0)\n",
    "                score += sim * (br * 5 + cr)\n",
    "            scores[prop] = score\n",
    "        ranked_props = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        for prop, score in ranked_props:\n",
    "            results.append({'srch_id': srch_id, 'prop_id': prop, 'score': score})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Limit validation set to 5000 search IDs\n",
    "val_data = val_data[val_data['srch_id'].isin(val_data['srch_id'].unique()[:5000])]\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "ranked_df = score_with_knn(val_data, similarity_matrix)\n",
    "print(f'Time to score 5000 validation searches: {time.time() - start_time:.2f} seconds')\n",
    "\n",
    "# Attach relevance\n",
    "true_labels = val_data[['srch_id', 'prop_id', 'click_bool', 'booking_bool']]\n",
    "ranked_df = ranked_df.merge(true_labels, on=['srch_id', 'prop_id'], how='left')\n",
    "ranked_df['relevance'] = ranked_df['booking_bool'] * 5 + ranked_df['click_bool']\n",
    "\n",
    "# NDCG@5\n",
    "ndcg_scores = []\n",
    "for srch_id, group in ranked_df.groupby('srch_id'):\n",
    "    sorted_group = group.sort_values('score', ascending=False)\n",
    "    ndcg_scores.append(ndcg_at_k(sorted_group['relevance'].values, k=5))\n",
    "\n",
    "print(f\"Validation NDCG@5: {np.mean(ndcg_scores):.4f}\")\n",
    "\n",
    "# Save output\n",
    "ranked_df.to_csv('ranked_knn_imputed_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ded9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ submission.csv created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load test set\n",
    "test_df = pd.read_parquet('test_set_processed.parquet')\n",
    "\n",
    "# Filter to only props included in training similarity matrix\n",
    "test_df = test_df[test_df['prop_id'].isin(similarity_matrix.index)]\n",
    "\n",
    "# Score function using precomputed_neighbors, click_rate, and book_rate\n",
    "def score_with_knn_test(test_df, precomputed_neighbors):\n",
    "    results = []\n",
    "    for srch_id, group in test_df.groupby('srch_id'):\n",
    "        props = group['prop_id'].values\n",
    "        scores = {}\n",
    "        for prop in props:\n",
    "            if prop not in precomputed_neighbors:\n",
    "                scores[prop] = 0\n",
    "                continue\n",
    "            top_k = precomputed_neighbors.get(prop, [])\n",
    "            score = 0\n",
    "            for neighbor, sim in top_k:\n",
    "                cr = click_rate.get(neighbor, 0)\n",
    "                br = book_rate.get(neighbor, 0)\n",
    "                score += sim * (br * 5 + cr)\n",
    "            scores[prop] = score\n",
    "        ranked_props = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        for rank, (prop, score) in enumerate(ranked_props):\n",
    "            results.append({'srch_id': srch_id, 'prop_id': prop, 'rank': rank})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Score and export\n",
    "ranked_test = score_with_knn_test(test_df, precomputed_neighbors)\n",
    "ranked_test.sort_values(['srch_id', 'rank'])[['srch_id', 'prop_id']].to_csv('submission5.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
